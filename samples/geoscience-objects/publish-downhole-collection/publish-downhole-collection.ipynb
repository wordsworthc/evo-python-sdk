{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish a Downhole Collection object\n",
    "\n",
    "This example shows how to convert drilling data in CSV format into an Evo geoscience object using the Evo Python SDK.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "You must have a Seequent account with the Evo entitlement to use this notebook.\n",
    "\n",
    "The following parameters must be provided:\n",
    "\n",
    "- The client ID of your Evo application.\n",
    "- The callback/redirect URL of your Evo application.\n",
    "\n",
    "To obtain these app credentials, refer to the [Apps and tokens guide](https://developer.seequent.com/docs/guides/getting-started/apps-and-tokens) in the Seequent Developer Portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "import pandas as pd\n",
    "from evo_schemas.components import (\n",
    "    BoundingBox_V1_0_1,\n",
    "    CategoryAttribute_V1_1_0,\n",
    "    CategoryData_V1_0_1,\n",
    "    ContinuousAttribute_V1_1_0,\n",
    "    Crs_V1_0_1_EpsgCode,\n",
    "    DistanceTable_V1_2_0_Distance,\n",
    "    Intervals_V1_0_1,\n",
    "    IntervalTable_V1_2_0_FromTo,\n",
    "    NanCategorical_V1_0_1,\n",
    "    NanContinuous_V1_0_1,\n",
    ")\n",
    "from evo_schemas.elements import (\n",
    "    FloatArray1_V1_0_1,\n",
    "    FloatArray2_V1_0_1,\n",
    "    FloatArray3_V1_0_1,\n",
    "    IntegerArray1_V1_0_1,\n",
    "    LookupTable_V1_0_1,\n",
    ")\n",
    "from evo_schemas.objects import (\n",
    "    DownholeCollection_V1_2_0,\n",
    "    DownholeCollection_V1_2_0_Collections_DistanceTable,\n",
    "    DownholeCollection_V1_2_0_Collections_IntervalTable,\n",
    "    DownholeCollection_V1_2_0_Location,\n",
    "    DownholeCollection_V1_2_0_Location_Holes,\n",
    "    DownholeCollection_V1_2_0_Location_Path,\n",
    ")\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "from evo.notebooks import FeedbackWidget, ServiceManagerWidget\n",
    "from evo.objects import ObjectAPIClient\n",
    "\n",
    "cache_location = \"data\"\n",
    "input_path = f\"{cache_location}/input\"\n",
    "\n",
    "# Evo app credentials\n",
    "client_id = \"<client_id>\"\n",
    "redirect_url = \"<redirect_url>\"\n",
    "\n",
    "manager = await ServiceManagerWidget.with_auth_code(\n",
    "    discovery_url=\"https://discover.api.seequent.com\",\n",
    "    redirect_url=redirect_url,\n",
    "    client_id=client_id,\n",
    "    cache_location=cache_location,\n",
    ").login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Evo Python SDK to create an object client and a data client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The object client will manage your auth token and Geoscience Object API requests.\n",
    "object_client = ObjectAPIClient(manager.get_environment(), manager.get_connector())\n",
    "\n",
    "# The data client will manage saving your data as Parquet and publishing your data to Evo storage.\n",
    "data_client = object_client.get_data_client(manager.cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define helper functions\n",
    "\n",
    "These functions assist with assembling the elements and components of geoscience objects and for viewing the new object in the Evo portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def create_hole_id_mapping(hole_id_table, value_list):\n",
    "    \"\"\"\n",
    "    Create a hole ID mapping table based on the hole ID table and the value list.\n",
    "\n",
    "    Args:\n",
    "        hole_id_table (pd.DataFrame): The hole ID lookup table.\n",
    "        value_list (pd.DataFrame): The value list to create the mapping from.\n",
    "\n",
    "    Returns:\n",
    "        mapping_df (pd.DataFrame): The hole ID mapping table.\n",
    "    \"\"\"\n",
    "\n",
    "    num_keys = len(hole_id_table.index)\n",
    "\n",
    "    mapping_df = pd.DataFrame(list())\n",
    "    mapping_df[\"hole_index\"] = hole_id_table[\"key\"]\n",
    "    mapping_df[\"offset\"] = [0] * num_keys\n",
    "    mapping_df[\"count\"] = [0] * num_keys\n",
    "\n",
    "    mapping_df[\"hole_index\"] = mapping_df[\"hole_index\"].astype(\"int32\")\n",
    "    mapping_df[\"offset\"] = mapping_df[\"offset\"].astype(\"uint64\")\n",
    "    mapping_df[\"count\"] = mapping_df[\"count\"].astype(\"uint64\")\n",
    "\n",
    "    prev_value = \"\"\n",
    "    key = \"\"\n",
    "    count = 0\n",
    "    offset = 0\n",
    "\n",
    "    for index, row in value_list.iterrows():\n",
    "        new_value = row[\"data\"]\n",
    "\n",
    "        if new_value != prev_value:\n",
    "            if prev_value != \"\":\n",
    "                mapping_df.loc[mapping_df[\"hole_index\"] == key, \"count\"] = count\n",
    "                mapping_df.loc[mapping_df[\"hole_index\"] == key, \"offset\"] = offset\n",
    "                offset += count\n",
    "\n",
    "            mask = hole_id_table[\"value\"] == new_value\n",
    "            masked_df = hole_id_table[mask]\n",
    "            try:\n",
    "                key_row = masked_df.iloc[[0]]\n",
    "            except IndexError:\n",
    "                print(\"Ignoring this hole ID\")\n",
    "                continue\n",
    "\n",
    "            key = key_row[\"key\"].iloc[0]\n",
    "            count = 1\n",
    "            prev_value = new_value\n",
    "        else:\n",
    "            count += 1\n",
    "\n",
    "    mapping_df.loc[mapping_df[\"hole_index\"] == key, \"count\"] = count\n",
    "    mapping_df.loc[mapping_df[\"hole_index\"] == key, \"offset\"] = offset\n",
    "\n",
    "    return mapping_df\n",
    "\n",
    "\n",
    "def create_category_lookup_and_values(attribute):\n",
    "    \"\"\"\n",
    "    Create a category lookup table and the associated column of mapped key values.\n",
    "\n",
    "    Args:\n",
    "        attribute (pd.DataFrame): An attribute of a geoscience object.\n",
    "\n",
    "    Returns:\n",
    "        table_df (pd.DataFrame): The category lookup table.\n",
    "        values_df (pd.DataFrame): The associated column with mapped key values.\n",
    "    \"\"\"\n",
    "\n",
    "    # Replace NaN with empty string\n",
    "    attribute.replace(np.nan, \"\", regex=True, inplace=True)\n",
    "    set_obj = set(attribute[\"data\"])\n",
    "    list_obj = list(set_obj)\n",
    "    list_obj.sort()\n",
    "    num_unique_elements = len(list_obj)\n",
    "\n",
    "    # Create lookup table\n",
    "    table_df = pd.DataFrame([])\n",
    "    table_df[\"key\"] = list(range(1, num_unique_elements + 1))\n",
    "    table_df[\"value\"] = list_obj\n",
    "\n",
    "    # Create data column\n",
    "    values_df = pd.DataFrame([])\n",
    "    values_df[\"data\"] = attribute[\"data\"].map(table_df.set_index(\"value\")[\"key\"])\n",
    "    return table_df, values_df\n",
    "\n",
    "\n",
    "def build_portal_url(object_metadata):\n",
    "    \"\"\"\n",
    "    Build the URL to view the object in the Evo Portal.\n",
    "\n",
    "    Args:\n",
    "        object_metadata (str): The object metadata object returned after the object is created.\n",
    "\n",
    "    Returns:\n",
    "        str: The URL to view the object in the Evo Portal.\n",
    "    \"\"\"\n",
    "\n",
    "    hub_url = object_metadata.environment.hub_url\n",
    "    hub_name = hub_url.split(\"://\")[1].split(\".\")[0]\n",
    "    org_id = object_metadata.environment.org_id\n",
    "    workspace_id = object_metadata.environment.workspace_id\n",
    "    object_id = object_metadata.id\n",
    "\n",
    "    url = f\"https://evo.seequent.com/{org_id}/workspaces/{hub_name}/{workspace_id}/viewer?id={object_id}\"\n",
    "\n",
    "    display(HTML(f'<a href=\"{url}\" target=\"_blank\">View object in the Evo Portal</a>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define object metadata\n",
    "\n",
    "Geoscience object data must conform to a specific object schema. The `evo-schemas` package provides Pydantic models that make it easy to work with the equivalent JSON schemas. \n",
    "For this example we'll use v1.2.0 of the downhole-collection schema, via the relevant Pydantic model.\n",
    "\n",
    "Enter values for these parameters that are required by the object schema.\n",
    "- `object_hole_id`: The column name that represents your hole ID. This value should be the same across all input files.\n",
    "- `object_name`: The name of the object.\n",
    "- `object_path`: The file path where the object will be found.\n",
    "- `object_epsg_code`: (Optional) The EPSG region code that matches the location of your data. Leave as `None` if not required.\n",
    "- `object_tags`: (Optional) A dictionary of additional tags to be assigned to the object. Leave as `None` is not required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the name of the hole ID parameter in the data.\n",
    "object_hole_id = \"BHID\"\n",
    "\n",
    "# Set other object properties.\n",
    "object_name = \"DHC_SDK_demo\"\n",
    "object_path = \"Jupyter_Example\"\n",
    "object_epsg_code = 32650\n",
    "object_tags = {\"Source\": \"Jupyter Notebook\"}\n",
    "\n",
    "# Define a coordinate reference system (CRS) for the object.\n",
    "coordinate_reference_system = Crs_V1_0_1_EpsgCode(epsg_code=object_epsg_code)\n",
    "\n",
    "# Create an empty list to store geoscience object collections.\n",
    "collections = []\n",
    "\n",
    "# Define the object path.\n",
    "full_obj_path = f\"{object_path}/{object_name}.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define object attributes and keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all of the attributes to be included in the object. Every attribute must have a unique key associated with it.\n",
    "# Keys must be unique across the entire object, and we recommend saving a reference to the keys for later use.\n",
    "collar_attributes_ref = {\n",
    "    \"Hole Type\": str(uuid.uuid4()),\n",
    "    \"Random Number\": str(uuid.uuid4()),\n",
    "}\n",
    "\n",
    "survey_attributes_ref = {\n",
    "    \"Random Numeric\": str(uuid.uuid4()),\n",
    "}\n",
    "\n",
    "object_attributes_ref = {\n",
    "    \"Wolfpass_WP_assay\": {\n",
    "        \"CU_pct\": str(uuid.uuid4()),\n",
    "        \"AU_gpt\": str(uuid.uuid4()),\n",
    "        \"DENSITY\": str(uuid.uuid4()),\n",
    "    },\n",
    "    \"Wolfpass_WP_lith\": {\n",
    "        \"ROCK\": str(uuid.uuid4()),\n",
    "        \"grouped_lith\": str(uuid.uuid4()),\n",
    "        \"Split_Dykes\": str(uuid.uuid4()),\n",
    "    },\n",
    "}\n",
    "\n",
    "depth_attribute_ref = {\n",
    "    \"Wolfpass_depths\": {\n",
    "        \"ATTR\": str(uuid.uuid4()),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output file paths.\n",
    "input_file_path = f\"{input_path}/Wolfpass_collar.csv\"\n",
    "\n",
    "# Load the collar file, count the number of hole IDs and sort the data based on the hole ID.\n",
    "input_df = pd.read_csv(input_file_path)\n",
    "num_hole_ids = len(input_df.index)\n",
    "sorted_collar_df = input_df.sort_values([object_hole_id]).reset_index(drop=True)\n",
    "\n",
    "sorted_collar_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hole ID table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most components in our downhole collection object will reference the hole IDs defined in the collar table.\n",
    "\n",
    "This means that we must create a 2-column dataframe that maps a unique `key` to a `hole ID`.\n",
    "\n",
    "The `location` component of the downhole collection object makes use of this mapping, so we provide a 1-column dataframe that lists the keys in the order they are displayed in the input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe for the hole IDs.\n",
    "hole_id_table_df = pd.DataFrame()\n",
    "hole_id_table_df[\"key\"] = [i for i in range(1, num_hole_ids + 1)]\n",
    "hole_id_table_df[\"value\"] = sorted_collar_df[object_hole_id]\n",
    "hole_id_table_component = LookupTable_V1_0_1.from_dict(data_client.save_dataframe(hole_id_table_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hole ID values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe and generate a list from 1 to `n`, where `n` is the number of hole IDs.\n",
    "# This table represents the list of hole IDs created in the previous step.\n",
    "hole_id_values_df = pd.DataFrame()\n",
    "hole_id_values_df[\"data\"] = [i for i in range(1, num_hole_ids + 1)]\n",
    "hole_id_values_component = IntegerArray1_V1_0_1.from_dict(data_client.save_dataframe(hole_id_values_df))\n",
    "\n",
    "hole_id_component = CategoryData_V1_0_1(\n",
    "    table=hole_id_table_component,\n",
    "    values=hole_id_values_component,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe and copy the required columns.\n",
    "# NOTE: The columns must be renamed to `x`, `y` and `z`.\n",
    "coordinates_df = pd.DataFrame()\n",
    "coordinates_df[[\"x\", \"y\", \"z\"]] = input_df[[\"XCOLLAR\", \"YCOLLAR\", \"ZCOLLAR\"]]\n",
    "\n",
    "# Create a `BoundingBox_V1_0_1` component for the bounding box.\n",
    "bounding_box = BoundingBox_V1_0_1(\n",
    "    min_x=coordinates_df[\"x\"].min(),\n",
    "    max_x=coordinates_df[\"x\"].max(),\n",
    "    min_y=coordinates_df[\"y\"].min(),\n",
    "    max_y=coordinates_df[\"y\"].max(),\n",
    "    min_z=coordinates_df[\"z\"].min(),\n",
    "    max_z=coordinates_df[\"z\"].max(),\n",
    ")\n",
    "\n",
    "# Create a `FloatArray3_V1_0_1` element for the coordinates.\n",
    "coordinates_component = FloatArray3_V1_0_1.from_dict(data_client.save_dataframe(coordinates_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the collar attributes\n",
    "\n",
    "collar_attributes = []\n",
    "\n",
    "for heading_name, heading_key in collar_attributes_ref.items():\n",
    "    print(heading_name)\n",
    "    if heading_name == \"Random Number\":\n",
    "        # Construct a FloatArray1 component.\n",
    "        values_df = pd.DataFrame()\n",
    "        values_df = input_df.loc[:, [heading_name]].copy().astype(float).reset_index(drop=True)\n",
    "        values = FloatArray1_V1_0_1.from_dict(data_client.save_dataframe(values_df))\n",
    "\n",
    "        attribute = ContinuousAttribute_V1_1_0(\n",
    "            name=heading_name, key=heading_key, nan_description=NanContinuous_V1_0_1(values=[]), values=values\n",
    "        )\n",
    "\n",
    "        collar_attributes.append(attribute)\n",
    "    elif heading_name == \"Hole Type\":\n",
    "        # Construct a LookupTable and IntegerArray1 component.\n",
    "        data_df = input_df.loc[:, [heading_name]].copy().reset_index(drop=True)\n",
    "        data_df.rename(columns={heading_name: \"data\"}, inplace=True)\n",
    "        table_df, values_df = create_category_lookup_and_values(data_df)\n",
    "        table = LookupTable_V1_0_1.from_dict(data_client.save_dataframe(table_df))\n",
    "        values = IntegerArray1_V1_0_1.from_dict(data_client.save_dataframe(values_df.astype(\"int32\")))\n",
    "\n",
    "        attribute = CategoryAttribute_V1_1_0(\n",
    "            name=heading_name,\n",
    "            key=heading_key,\n",
    "            nan_description=NanCategorical_V1_0_1(values=[]),\n",
    "            table=table,\n",
    "            values=values,\n",
    "        )\n",
    "\n",
    "        collar_attributes.append(attribute)\n",
    "\n",
    "print(collar_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a distances dataframe and copy the required columns.\n",
    "# NOTE: The downhole collection object requires 3 columns: `final`, `target` and `current`,\n",
    "# but in this data set there is only 1 depth value.\n",
    "# To solve this problem we duplicate the depth value across the other 2 columns.\n",
    "distances_df = pd.DataFrame()\n",
    "distances_df[\"final\"] = input_df[\"maxdepth\"]\n",
    "distances_df[\"target\"] = distances_df.loc[:, \"final\"]\n",
    "distances_df[\"current\"] = distances_df.loc[:, \"final\"]\n",
    "\n",
    "# Create a `FloatArray3_V1_0_1` element for the distances.\n",
    "distances_component = FloatArray3_V1_0_1.from_dict(data_client.save_dataframe(distances_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input and output file paths.\n",
    "input_file_path = f\"{input_path}/Wolfpass_survey.csv\"\n",
    "\n",
    "# Load the survey file.\n",
    "df = pd.read_csv(input_file_path)\n",
    "df = df.sort_values([object_hole_id]).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove hole IDs from the survey file that don't appear in the hole ID table.\n",
    "# If you leave these rogue hole IDs in your survey table Leapfrog won't be able to import your geoscience object.\n",
    "df.drop(df[~df[object_hole_id].isin(list(hole_id_table_df[\"value\"]))].index, inplace=True)\n",
    "df.reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe and copy the required survey column.\n",
    "survey_values_df = pd.DataFrame()\n",
    "survey_values_df[\"data\"] = df[object_hole_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the survey attributes\n",
    "\n",
    "# Construct a FloatArray1 component for each attribute column in the survey table.\n",
    "survey_attributes = []\n",
    "\n",
    "for heading_name, heading_key in survey_attributes_ref.items():\n",
    "    values_df = pd.DataFrame()\n",
    "    values_df = df.loc[:, [heading_name]].copy().astype(float).reset_index(drop=True)\n",
    "    values = FloatArray1_V1_0_1.from_dict(data_client.save_dataframe(values_df))\n",
    "\n",
    "    attribute = ContinuousAttribute_V1_1_0(\n",
    "        name=heading_name, key=heading_key, nan_description=NanContinuous_V1_0_1(values=[]), values=values\n",
    "    )\n",
    "\n",
    "    survey_attributes.append(attribute)\n",
    "\n",
    "print(survey_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the column names to what is expected in the geoscience object and drop the hold ID column.\n",
    "df = df.rename(columns={\"DIP\": \"dip\", \"AT\": \"distance\", \"BRG\": \"azimuth\"})\n",
    "# Drop the hole ID column and all columns listed in survey_attributes\n",
    "cols_to_drop = [object_hole_id] + list(survey_attributes_ref.keys())\n",
    "df = df.drop(columns=cols_to_drop, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swap the column order to match what the geoscience object\n",
    "# requires (distance/azimuth/dip) and save the result in a new dataframe.\n",
    "# NOTE: If your data set already has the correct ordering you can skip this step.\n",
    "path_df = df.iloc[:, [0, 2, 1]]\n",
    "path_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a `DownholeCollection_V1_2_0_Location_Path` object to handle the *path* component.\n",
    "path_component = DownholeCollection_V1_2_0_Location_Path.from_dict(data_client.save_dataframe(path_df))\n",
    "path_component.attributes = survey_attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the hole ID table from earlier, create a mapping between\n",
    "# the hole IDs defined in the collar table and the ones found in the survey table.\n",
    "holes_df = create_hole_id_mapping(hole_id_table=hole_id_table_df, value_list=survey_values_df)\n",
    "holes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a `DownholeCollection_V1_2_0_Location_Holes` object to handle the *holes* component.\n",
    "holes_component = DownholeCollection_V1_2_0_Location_Holes.from_dict(data_client.save_dataframe(holes_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a `DownholeCollection_V1_2_0_Location` object which combines all\n",
    "# of the components created so far in this section.\n",
    "location_component = DownholeCollection_V1_2_0_Location(\n",
    "    distances=distances_component,\n",
    "    holes=holes_component,\n",
    "    hole_id=hole_id_component,\n",
    "    path=path_component,\n",
    "    coordinates=coordinates_component,\n",
    "    attributes=collar_attributes,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the name of the input csv file (without the file extension) in the `collection_name` variable.\n",
    "# The same name will be applied to this collection in the new geoscience object, but this is configurable.\n",
    "collection_name = \"Wolfpass_depths\"\n",
    "input_file_path = f\"{input_path}/{collection_name}.csv\"\n",
    "\n",
    "# Load the assay file and find the length.\n",
    "df = pd.read_csv(input_file_path)\n",
    "orig_count = len(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an **depth_bhid_values** dataframe and copy the hole IDs.\n",
    "depth_bhid_values_df = pd.DataFrame()\n",
    "depth_bhid_values_df[\"data\"] = df[object_hole_id]\n",
    "depth_bhid_values_df.sort_values(by=\"data\", inplace=True)\n",
    "depth_bhid_values_df.reset_index(drop=True, inplace=True)\n",
    "depth_bhid_values_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe and copy the *DEPTH* column to it.\n",
    "depth_interval_table_df = df.loc[:, [\"DEPTH\"]].copy().reset_index(drop=True)\n",
    "depth_interval_table_df.columns = [\"data\"]\n",
    "depth_interval_table_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a component that represents the depths.\n",
    "depths = FloatArray1_V1_0_1.from_dict(data_client.save_dataframe(depth_interval_table_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Holes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping between the hole IDs in the depth table and the hole IDs from the collar table.\n",
    "depth_holes_df = create_hole_id_mapping(hole_id_table=hole_id_table_df, value_list=depth_bhid_values_df)\n",
    "depth_holes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a component that represents the depth holes.\n",
    "location_holes_component = DownholeCollection_V1_2_0_Location_Holes.from_dict(\n",
    "    data_client.save_dataframe(depth_holes_df)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Depth Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a FloatArray1 component for each column in the assay table.\n",
    "attributes = []\n",
    "\n",
    "for heading_name, heading_key in depth_attribute_ref[\"Wolfpass_depths\"].items():\n",
    "    values_df = pd.DataFrame()\n",
    "    values_df = df.loc[:, [heading_name]].copy().astype(float).reset_index(drop=True)\n",
    "    values = FloatArray1_V1_0_1.from_dict(data_client.save_dataframe(values_df))\n",
    "\n",
    "    attribute = ContinuousAttribute_V1_1_0(\n",
    "        name=heading_name, key=heading_key, nan_description=NanContinuous_V1_0_1(values=[]), values=values\n",
    "    )\n",
    "\n",
    "    attributes.append(attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the `DownholeCollection_V1_2_0_Collections_DistanceTable` object by combining the *holes* and *distance* components.\n",
    "collection = DownholeCollection_V1_2_0_Collections_DistanceTable(\n",
    "    name=collection_name,\n",
    "    holes=location_holes_component,\n",
    "    distance=DistanceTable_V1_2_0_Distance(values=depths, attributes=attributes),\n",
    ")\n",
    "\n",
    "collections.append(collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the name of the input csv file (without the file extension) in the `collection_name` variable.\n",
    "# The same name will be applied to this collection in the new geoscience object, but this is configurable.\n",
    "collection_name = \"Wolfpass_WP_assay\"\n",
    "input_file_path = f\"{input_path}/{collection_name}.csv\"\n",
    "\n",
    "# Load the assay file and find the length.\n",
    "df = pd.read_csv(input_file_path)\n",
    "orig_count = len(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove hole IDs from the survey file that don't appear in the hole ID table.\n",
    "# If you leave these rogue hole IDs in your survey table Leapfrog won't be able to import your geoscience object.\n",
    "df.drop(df[~df[object_hole_id].isin(list(hole_id_table_df[\"value\"]))].index, inplace=True)\n",
    "df.reset_index(drop=True)\n",
    "new_count = len(df)\n",
    "print(f\"Num IDs removed: {orig_count - new_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some rows in the example assay table contain invalid characters,\n",
    "# eg. the AU_gpt column contains the string *NS* instead of a floating point number.\n",
    "# In this example we replace the *NS* string with a value of *-999.0*.\n",
    "# The table also contains some *less than* symbols (ie. '<') which also need to be removed.\n",
    "df = df.replace(to_replace=\"NS\", value=-999.0)\n",
    "df = df.loc[df[\"AU_gpt\"].str[:1] != \"<\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to create a look-up table but the hole IDs must be grouped and ordered, so we sort the dataframe accordingly.\n",
    "# We also count the number of rows in the final dataframe.\n",
    "df.sort_values(by=[object_hole_id], inplace=True)\n",
    "num_rows = len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an **assay_bhid_values** dataframe and copy the hole IDs.\n",
    "assay_bhid_values_df = pd.DataFrame()\n",
    "assay_bhid_values_df[\"data\"] = df[object_hole_id]\n",
    "assay_bhid_values_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe and copy the *FROM* and *TO* columns to it.\n",
    "assay_interval_table_df = df.loc[:, [\"FROM\", \"TO\"]].copy().reset_index(drop=True)\n",
    "assay_interval_table_df.columns = [\"data1\", \"data2\"]\n",
    "assay_interval_table_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a component that represents the intervals.\n",
    "start_and_end = FloatArray2_V1_0_1.from_dict(data_client.save_dataframe(assay_interval_table_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping between the hole IDs in the assay table and the hole IDs from the collar table.\n",
    "assay_holes_df = create_hole_id_mapping(hole_id_table=hole_id_table_df, value_list=assay_bhid_values_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a component that represents the assay holes.\n",
    "location_holes_component = DownholeCollection_V1_2_0_Location_Holes.from_dict(\n",
    "    data_client.save_dataframe(assay_holes_df)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collection Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a FloatArray1 component for each column in the assay table.\n",
    "attributes = []\n",
    "\n",
    "for heading_name, heading_key in object_attributes_ref[\"Wolfpass_WP_assay\"].items():\n",
    "    values_df = pd.DataFrame()\n",
    "    values_df = df.loc[:, [heading_name]].copy().astype(float).reset_index(drop=True)\n",
    "    values = FloatArray1_V1_0_1.from_dict(data_client.save_dataframe(values_df))\n",
    "\n",
    "    attribute = ContinuousAttribute_V1_1_0(\n",
    "        name=heading_name, key=heading_key, nan_description=NanContinuous_V1_0_1(values=[]), values=values\n",
    "    )\n",
    "\n",
    "    attributes.append(attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the `DownholeCollection_V1_2_0_Collections_IntervalTable` object by combining the *holes* and *from_to* components.\n",
    "from_to_component = IntervalTable_V1_2_0_FromTo(\n",
    "    intervals=Intervals_V1_0_1(start_and_end=start_and_end),\n",
    "    attributes=attributes,\n",
    ")\n",
    "\n",
    "collection = DownholeCollection_V1_2_0_Collections_IntervalTable(\n",
    "    name=collection_name, from_to=from_to_component, holes=location_holes_component\n",
    ")\n",
    "\n",
    "collections.append(collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lithology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the name of the input csv file (without the file extension) in the `collection_name` variable.\n",
    "# The same name will be applied to this collection in the new geoscience object, but this is configurable.\n",
    "collection_name = \"Wolfpass_WP_lith\"\n",
    "input_file_path = f\"{input_path}/{collection_name}.csv\"\n",
    "\n",
    "# Load the lithology file and find the length.\n",
    "df = pd.read_csv(input_file_path)\n",
    "orig_count = len(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove hole IDs from the survey file that don't appear in the hole ID table.\n",
    "# If you leave these rogue hole IDs in your survey table Leapfrog won't be able to import your geoscience object.\n",
    "df.drop(df[~df[object_hole_id].isin(list(hole_id_table_df[\"value\"]))].index, inplace=True)\n",
    "df.reset_index(drop=True)\n",
    "new_count = len(df)\n",
    "print(f\"Num IDs removed: {orig_count - new_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to create a look-up table but the hole IDs must be grouped and ordered, so we sort the dataframe accordingly.\n",
    "# We also count the number of rows in the final dataframe.\n",
    "df.sort_values(by=[object_hole_id], inplace=True)\n",
    "num_rows = len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a **lith_bhid_values** dataframe and copy the hole IDs.\n",
    "lith_bhid_values_df = pd.DataFrame()\n",
    "lith_bhid_values_df[\"data\"] = df[object_hole_id]\n",
    "lith_bhid_values_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe and copy the `FROM` and `TO` columns to it.\n",
    "lith_interval_table_df = pd.DataFrame()\n",
    "lith_interval_table_df = df.loc[:, [\"FROM\", \"TO\"]].copy().reset_index(drop=True)\n",
    "lith_interval_table_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a component that presents the intervals.\n",
    "start_and_end = FloatArray2_V1_0_1.from_dict(data_client.save_dataframe(lith_interval_table_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping between the hole IDs in the lithology table and the hole IDs from the collar table.\n",
    "lith_holes_df = create_hole_id_mapping(hole_id_table=hole_id_table_df, value_list=lith_bhid_values_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a component that represents the lithology holes.\n",
    "location_holes_component = DownholeCollection_V1_2_0_Location_Holes.from_dict(data_client.save_dataframe(lith_holes_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collection Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a FloatArray1 component for each column in the assay table.\n",
    "attributes = []\n",
    "\n",
    "for heading_name, heading_key in object_attributes_ref[\"Wolfpass_WP_lith\"].items():\n",
    "    lith_df = pd.DataFrame()\n",
    "    lith_df[\"data\"] = df[heading_name]\n",
    "    lith_df = lith_df.astype(str).reset_index(drop=True)\n",
    "    table_df, values_df = create_category_lookup_and_values(lith_df)\n",
    "\n",
    "    table = LookupTable_V1_0_1.from_dict(data_client.save_dataframe(table_df))\n",
    "    values = IntegerArray1_V1_0_1.from_dict(data_client.save_dataframe(values_df))\n",
    "\n",
    "    attribute = CategoryAttribute_V1_1_0(\n",
    "        name=heading_name, key=heading_key, nan_description=NanCategorical_V1_0_1(values=[]), table=table, values=values\n",
    "    )\n",
    "    attributes.append(attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the `DownholeCollection_V1_2_0_Collections_IntervalTable` object by combining the *holes* and *from_to* components.\n",
    "from_to_component = IntervalTable_V1_2_0_FromTo(\n",
    "    intervals=Intervals_V1_0_1(\n",
    "        start_and_end=start_and_end,\n",
    "    ),\n",
    "    attributes=attributes,\n",
    ")\n",
    "\n",
    "collection = DownholeCollection_V1_2_0_Collections_IntervalTable(\n",
    "    name=collection_name, from_to=from_to_component, holes=location_holes_component\n",
    ")\n",
    "\n",
    "collections.append(collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assemble the downhole collection object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly, assemble the complete geoscience object by combining all previously defined components.\n",
    "# - The name and UUID are used to identify the object.\n",
    "# - The UUID is set to None because this is a new object. A new UUID will be assigned by the Evo service.\n",
    "# - The bounding box defines the spatial extent of the object.\n",
    "# - The tags provide metadata about the object.\n",
    "# - The coordinate reference system defines the spatial reference for the object.\n",
    "# - The locations component contains the coordinates and attributes.\n",
    "downhole_collection = DownholeCollection_V1_2_0(\n",
    "    name=object_name,\n",
    "    uuid=None,\n",
    "    bounding_box=bounding_box,\n",
    "    tags=object_tags,\n",
    "    coordinate_reference_system=coordinate_reference_system,\n",
    "    location=location_component,\n",
    "    collections=collections,\n",
    ")\n",
    "\n",
    "await data_client.upload_referenced_data(downhole_collection.as_dict(), FeedbackWidget(\"Uploading data\"))\n",
    "new_downhole_collection_metadata = await object_client.create_geoscience_object(\n",
    "    full_obj_path, downhole_collection.as_dict()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View the object in the Evo portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_portal_url(new_downhole_collection_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! You now have a new geoscience object in Evo containing your downhole-collection data.\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this example, we've completed the following:\n",
    "* Analysed the collar and survey tables and constructed the elements and components required for locations.\n",
    "* Analysed the data columns and constructed the elements and components required for attributes.\n",
    "* Converted the input location and attribute data into Parquet format and saved it to the local cache.\n",
    "* Combined all of the elements, components and data references into the downhole-collection schema format.\n",
    "* Uploaded the Parquet files and the newly assembled object in JSON format to Evo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
