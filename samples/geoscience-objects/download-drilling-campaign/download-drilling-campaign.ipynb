{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "## Download a Drilling Campaign object and save it in CSV format\n",
    "\n",
    "This example shows how to download a drilling-campaign object from an Evo workspace and how to construct CSV files from the data.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "You must have a Seequent account with the Evo entitlement to use this notebook.\n",
    "\n",
    "The following parameters must be provided:\n",
    "\n",
    "- The client ID of your Evo application.\n",
    "- The callback/redirect URL of your Evo application.\n",
    "\n",
    "To obtain these app credentials, refer to the [Apps and tokens guide](https://developer.seequent.com/docs/guides/getting-started/apps-and-tokens) in the Seequent Developer Portal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from evo.notebooks import FeedbackWidget, ServiceManagerWidget\n",
    "from evo.objects import ObjectAPIClient\n",
    "\n",
    "cache_location = \"data\"\n",
    "\n",
    "# Evo app credentials\n",
    "client_id = \"<your-client-id>\"\n",
    "redirect_url = \"<your-redirect-url>\"\n",
    "\n",
    "manager = await ServiceManagerWidget.with_auth_code(\n",
    "    discovery_url=\"https://discover.api.seequent.com\",\n",
    "    redirect_url=redirect_url,\n",
    "    client_id=client_id,\n",
    "    cache_location=cache_location,\n",
    ").login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Use the Evo Python SDK to create an object client and a data client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The object client will manage your auth token and Geoscience Object API requests.\n",
    "object_client = ObjectAPIClient(manager.get_environment(), manager.get_connector())\n",
    "\n",
    "# The data client will manage saving your data as Parquet and publishing your data to Evo storage.\n",
    "data_client = object_client.get_data_client(manager.cache)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### List all objects in the workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prettytable import PrettyTable\n",
    "\n",
    "all_objects = await object_client.list_all_objects()\n",
    "\n",
    "table = PrettyTable([\"Name\", \"Object ID\"])\n",
    "for index, obj in enumerate(all_objects):\n",
    "    if \"drilling-campaign\" in obj.schema_id.sub_classification:\n",
    "        table.add_row([obj.name.ljust(40), str(obj.id).ljust(40)])\n",
    "\n",
    "if len(table.rows) == 0:\n",
    "    print(\"No drilling campaigns found. Publish a drilling campaign using the 'publish-drilling-campaign' notebook.\")\n",
    "else:\n",
    "    print(\"Drilling campaigns found:\")\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Enter the `Object ID` value for the chosen drilling-campaign object in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_id = \"<your-object-id>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Download the Parquet files and assemble the CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "downloaded_object = await object_client.download_object_by_id(object_id=object_id)\n",
    "\n",
    "metadata = downloaded_object.metadata\n",
    "downloaded_dict = downloaded_object.as_dict()\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "\n",
    "def download_table(table_info, fb=None):\n",
    "    return data_client.download_table(\n",
    "        object_id=metadata.id, version_id=metadata.version_id, table_info=table_info, fb=fb\n",
    "    )\n",
    "\n",
    "\n",
    "# Use the data client to download the coordinate data.\n",
    "hole_indices = (\n",
    "    await download_table(\n",
    "        downloaded_dict[\"hole_id\"][\"values\"],\n",
    "        fb=FeedbackWidget(f\"Downloading hole indices data as '{downloaded_dict['hole_id']['values']['data']}'\"),\n",
    "    )\n",
    ").to_pandas()\n",
    "index_to_name_map = (\n",
    "    await download_table(\n",
    "        downloaded_dict[\"hole_id\"][\"table\"],\n",
    "        fb=FeedbackWidget(\n",
    "            f\"Downloading hole index to name map data as '{downloaded_dict['hole_id']['table']['data']}'\"\n",
    "        ),\n",
    "    )\n",
    ").to_pandas()\n",
    "\n",
    "names = pd.DataFrame({\"key\": hole_indices[\"data\"]}).merge(index_to_name_map, on=\"key\", how=\"left\")[\"value\"]\n",
    "names = names.rename(\"hole_id\")\n",
    "\n",
    "collar_locations = (\n",
    "    await download_table(\n",
    "        downloaded_dict[\"planned\"][\"collar\"][\"coordinates\"],\n",
    "        fb=FeedbackWidget(\n",
    "            f\"Downloading collar locations data as '{downloaded_dict['planned']['collar']['coordinates']['data']}'\"\n",
    "        ),\n",
    "    )\n",
    ").to_pandas()\n",
    "hole_lengths = (\n",
    "    await download_table(\n",
    "        downloaded_dict[\"planned\"][\"collar\"][\"distances\"],\n",
    "        fb=FeedbackWidget(\n",
    "            f\"Downloading collar hole distances table as '{downloaded_dict['planned']['collar']['distances']['data']}'\"\n",
    "        ),\n",
    "    )\n",
    ").to_pandas()\n",
    "chunk_data = (\n",
    "    await download_table(\n",
    "        downloaded_dict[\"planned\"][\"collar\"][\"holes\"],\n",
    "        fb=FeedbackWidget(\n",
    "            f\"Downloading collar chunks data as '{downloaded_dict['planned']['collar']['holes']['data']}'\"\n",
    "        ),\n",
    "    )\n",
    ").to_pandas()\n",
    "\n",
    "attributes = []\n",
    "# Use the data client to download the attribute data and merge it with the coordinates data.\n",
    "if \"attributes\" in downloaded_dict[\"planned\"][\"collar\"]:\n",
    "    for attribute in downloaded_dict[\"planned\"][\"collar\"][\"attributes\"]:\n",
    "        attribute_name = attribute[\"name\"]\n",
    "        attribute_type = attribute[\"attribute_type\"]\n",
    "\n",
    "        # Download the attribute data. Every attribute has a 'values' data file.\n",
    "        values_data = (\n",
    "            await data_client.download_table(\n",
    "                object_id=metadata.id,\n",
    "                version_id=metadata.version_id,\n",
    "                table_info=attribute[\"values\"],\n",
    "                fb=FeedbackWidget(\n",
    "                    f\"Downloading attribute '{attribute_name}' values data as '{attribute['values']['data']}'\"\n",
    "                ),\n",
    "            )\n",
    "        ).to_pandas()\n",
    "        values_data = values_data.rename(columns={\"data\": attribute_name})\n",
    "        attributes.append(values_data)\n",
    "\n",
    "df = pd.concat([names, collar_locations, hole_lengths, *attributes], axis=1)\n",
    "output_filename = Path(f\"{cache_location}/collar.csv\").resolve()\n",
    "df.to_csv(output_filename, index=False)\n",
    "print(\"Collar data saved to:\", output_filename)\n",
    "\n",
    "## Path\n",
    "path_data = (\n",
    "    await download_table(\n",
    "        downloaded_dict[\"planned\"][\"path\"],\n",
    "        fb=FeedbackWidget(f\"Downloading collar path data as '{downloaded_dict['planned']['path']['data']}'\"),\n",
    "    )\n",
    ").to_pandas()\n",
    "\n",
    "processed_path_data = pd.concat(\n",
    "    [\n",
    "        path_data.iloc[start : start + length].reset_index(drop=True)\n",
    "        for start, length in zip(chunk_data[\"offset\"], chunk_data[\"count\"])\n",
    "    ],\n",
    "    axis=0,\n",
    ").reset_index(drop=True)\n",
    "\n",
    "hole_name = []\n",
    "\n",
    "for hole_index, count in zip(chunk_data[\"hole_index\"], chunk_data[\"count\"]):\n",
    "    hole_name.extend([index_to_name_map[index_to_name_map[\"key\"] == hole_index][\"value\"].values[0]] * count)\n",
    "processed_path_data[\"hole_name\"] = hole_name\n",
    "processed_path_data = processed_path_data[\n",
    "    [\"hole_name\"] + [col for col in processed_path_data.columns if col != \"hole_name\"]\n",
    "]\n",
    "\n",
    "output_filename = Path(f\"{cache_location}/path.csv\").resolve()\n",
    "processed_path_data.to_csv(output_filename, index=False)\n",
    "print(\"Path data saved to:\", output_filename)\n",
    "\n",
    "## Collections\n",
    "attribute_tables = {}\n",
    "# Use the data client to download the attribute data and merge it with the coordinates data.\n",
    "if \"collections\" in downloaded_dict[\"planned\"]:\n",
    "    for attribute_table in downloaded_dict[\"planned\"][\"collections\"]:\n",
    "        collection_name = attribute_table[\"name\"]\n",
    "        collection_type = attribute_table[\"collection_type\"]\n",
    "\n",
    "        if collection_type == \"interval\":\n",
    "            distance_container = attribute_table[\"from_to\"][\"intervals\"]\n",
    "            attribute_container = attribute_table[\"from_to\"][\"attributes\"]\n",
    "            distance_data = (\n",
    "                await data_client.download_table(\n",
    "                    object_id=metadata.id,\n",
    "                    version_id=metadata.version_id,\n",
    "                    table_info=distance_container,\n",
    "                    fb=FeedbackWidget(f\"Downloading distance data as '{distance_container['data']}'\"),\n",
    "                )\n",
    "            ).to_pandas()\n",
    "        elif collection_type == \"distance\":\n",
    "            distance_container = attribute_table[\"distance\"][\"values\"]\n",
    "            attribute_container = attribute_table[\"distance\"][\"attributes\"]\n",
    "            distance_data = (\n",
    "                await data_client.download_table(\n",
    "                    object_id=metadata.id,\n",
    "                    version_id=metadata.version_id,\n",
    "                    table_info=distance_container,\n",
    "                    fb=FeedbackWidget(f\"Downloading distance data as '{distance_container['data']}'\"),\n",
    "                )\n",
    "            ).to_pandas()\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        columns = [distance_data]\n",
    "        for column in attribute_container:\n",
    "            attribute_name = column[\"name\"]\n",
    "            attribute_type = column[\"attribute_type\"]\n",
    "\n",
    "            # If the attribute is a category, download the 'table' data as well.\n",
    "            if attribute_type == \"category\":\n",
    "                table_data = await data_client.download_table(\n",
    "                    object_id=metadata.id,\n",
    "                    version_id=metadata.version_id,\n",
    "                    table_info=attribute[\"table\"],\n",
    "                    fb=FeedbackWidget(\n",
    "                        f\"Downloading attribute '{attribute_name}' table data as '{attribute['table']['data']}'\"\n",
    "                    ),\n",
    "                )\n",
    "\n",
    "            # Download the attribute data. Every attribute has a 'values' data file.\n",
    "            values_data = (\n",
    "                await data_client.download_table(\n",
    "                    object_id=metadata.id,\n",
    "                    version_id=metadata.version_id,\n",
    "                    table_info=column[\"values\"],\n",
    "                    fb=FeedbackWidget(\n",
    "                        f\"Downloading attribute '{attribute_name}' values data as '{column['values']['data']}'\"\n",
    "                    ),\n",
    "                )\n",
    "            ).to_pandas()\n",
    "\n",
    "            # Assemble the dataframe.\n",
    "            if attribute_type == \"category\":\n",
    "                # Merge the values data with the table data.\n",
    "                merged_data = pd.merge(values_data, table_data.to_pandas(), left_on=\"data\", right_on=\"key\", how=\"left\")\n",
    "                # Drop the 'data' and 'key' columns from the merged data.\n",
    "                merged_data.drop(columns=[\"data\", \"key\"], inplace=True)\n",
    "                # Rename the 'value' column to the attribute name.\n",
    "                merged_data.rename(columns={\"value\": attribute_name}, inplace=True)\n",
    "                # Concatenate the merged data with the coordinates data.\n",
    "                columns.append(merged_data)\n",
    "\n",
    "            elif attribute_type == \"scalar\":\n",
    "                data = values_data\n",
    "                # Rename the 'data' column to the attribute name.\n",
    "                data.rename(columns={\"data\": attribute_name}, inplace=True)\n",
    "                # Concatenate the data with the coordinates data.\n",
    "                columns.append(data)\n",
    "\n",
    "        merged_df = pd.concat(columns, axis=1)\n",
    "        processed_merged_df = pd.concat(\n",
    "            [\n",
    "                merged_df.iloc[start : start + length].reset_index(drop=True)\n",
    "                for start, length in zip(chunk_data[\"offset\"], chunk_data[\"count\"])\n",
    "            ],\n",
    "            axis=0,\n",
    "        ).reset_index(drop=True)\n",
    "\n",
    "        hole_name = []\n",
    "        for hole_index, count in zip(chunk_data[\"hole_index\"], chunk_data[\"count\"]):\n",
    "            hole_name.extend([index_to_name_map[index_to_name_map[\"key\"] == hole_index][\"value\"].values[0]] * count)\n",
    "        processed_merged_df[\"hole_name\"] = hole_name\n",
    "        processed_merged_df = processed_merged_df[\n",
    "            [\"hole_name\"] + [col for col in processed_merged_df.columns if col != \"hole_name\"]\n",
    "        ]\n",
    "        attribute_tables[collection_name] = processed_merged_df\n",
    "\n",
    "if attribute_tables:\n",
    "    for attribute in attribute_tables:\n",
    "        output_filename = Path(f\"{cache_location}/{attribute}.csv\").resolve()\n",
    "        attribute_tables[attribute].to_csv(output_filename, index=False)\n",
    "        print(f\"Collection '{attribute}' data saved to:\", output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Get the list of files in the cache_location directory (including subdirectories)\n",
    "downloaded_files = Path(cache_location).glob(\"**/*\")\n",
    "\n",
    "# Iterate through each file and rename to add '.parquet' extension\n",
    "for file_path in downloaded_files:\n",
    "    if (\n",
    "        file_path.is_file()\n",
    "        and not file_path.name.endswith(\".parquet\")\n",
    "        and not file_path.name.startswith(\".\")\n",
    "        and not file_path.suffix\n",
    "    ):  # Only rename files with no extension\n",
    "        new_path = file_path.with_suffix(file_path.suffix + \".parquet\")\n",
    "        file_path.rename(new_path)\n",
    "        print(f\"Renamed: {file_path.name} -> {new_path.name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
